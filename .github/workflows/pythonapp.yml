# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://help.github.com/actions/language-and-framework-guides/using-python-with-github-actions

name: Python application

on:
  push:
    branches: [feature/deploy-on-cpqd]

jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
      - name: Install VPNC
        run: |
          sudo apt-get install vpnc
      - name: Create VPN Configuration File
        run: |
          sudo bash -c 'cat <<EOF > /etc/vpnc/vpnc.conf
          IPSec gateway ${{ secrets.VPN_GATEWAY }}
          IPSec ID ${{ secrets.VPN_IPSEC_ID }}
          IPSec secret ${{ secrets.VPN_IPSEC_SECRET }}
          Xauth username ${{ secrets.VPN_USERNAME }}
          Xauth password ${{ secrets.VPN_PASSWORD }}
          EOF'
      - name: Connect to VPN
        run: |
          sudo vpnc vpnc
      - name: Install platiagro
        uses: appleboy/ssh-action@master
        with:
          host: 10.50.11.165
          username: ${{ secrets.SSH_USER }}
          password: ${{ secrets.SSH_PASSWORD }}
          port: 22
          script: |
            sudo kubeadm reset -f
            sudo docker system prune -a --volumes -f
            sudo rm -rf /l/disk0 /home/cpqd/platiagro
            for i in `seq 0 29`; do sudo mkdir -p "/l/disk0/disk-$i"; done
            cat images.txt|xargs -L 1 sudo docker pull
            export KUBEFLOW_MASTER_IP_ADDRESS=$(ifconfig|grep -Po 10.50.11.[0-9]+|head -n 1)

            sudo sysctl net.bridge.bridge-nf-call-iptables=1
            sudo kubeadm init

            rm -rf $HOME/.kube
            mkdir -p $HOME/.kube
            sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
            sudo chown $(id -u):$(id -g) $HOME/.kube/config
            kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
            kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=admin --user=kubelet --group=system:serviceaccounts
            kubectl taint nodes --all node-role.kubernetes.io/master-

            cat <<EOF | kubectl apply -f -
            kind: StorageClass
            apiVersion: storage.k8s.io/v1
            metadata:
              name: local-storage
            provisioner: kubernetes.io/no-provisioner
            reclaimPolicy: Delete
            EOF

            kubectl patch storageclass local-storage -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'

            cat <<EOF | kubectl apply -f -
            ---
            # Source: provisioner/templates/provisioner.yaml
            apiVersion: v1
            kind: ConfigMap
            metadata:
              name: local-provisioner-config
              namespace: default
              labels:
                heritage: "Tiller"
                release: "release-name"
                chart: provisioner-2.3.2
            data:
              storageClassMap: |
                local-storage:
                  hostDir: /mnt/disks
                  mountDir: /mnt/disks
                  blockCleanerCommand:
                    - "/scripts/shred.sh"
                    - "2"
                  volumeMode: Filesystem
            ---
            apiVersion: apps/v1
            kind: DaemonSet
            metadata:
              name: local-volume-provisioner
              namespace: default
              labels:
                app: local-volume-provisioner
                heritage: "Tiller"
                release: "release-name"
                chart: provisioner-2.3.2
            spec:
              selector:
                matchLabels:
                  app: local-volume-provisioner
              template:
                metadata:
                  labels:
                    app: local-volume-provisioner
                spec:
                  serviceAccountName: local-storage-admin
                  containers:
                    - image: "quay.io/external_storage/local-volume-provisioner:v2.3.2"
                      name: provisioner
                      securityContext:
                        privileged: true
                      env:
                      - name: MY_NODE_NAME
                        valueFrom:
                          fieldRef:
                            fieldPath: spec.nodeName
                      - name: MY_NAMESPACE
                        valueFrom:
                          fieldRef:
                            fieldPath: metadata.namespace
                      - name: JOB_CONTAINER_IMAGE
                        value: "quay.io/external_storage/local-volume-provisioner:v2.3.2"
                      volumeMounts:
                        - mountPath: /etc/provisioner/config
                          name: provisioner-config
                          readOnly: true
                        - mountPath: /dev
                          name: provisioner-dev
                        - mountPath: /mnt/disks
                          name: disks
                          mountPropagation: "HostToContainer"
                  volumes:
                    - name: provisioner-config
                      configMap:
                        name: local-provisioner-config
                    - name: provisioner-dev
                      hostPath:
                        path: /dev
                    - name: disks
                      hostPath:
                        path: /mnt/disks

            ---
            # Source: provisioner/templates/provisioner-service-account.yaml

            apiVersion: v1
            kind: ServiceAccount
            metadata:
              name: local-storage-admin
              namespace: default
              labels:
                heritage: "Tiller"
                release: "release-name"
                chart: provisioner-2.3.2

            ---
            # Source: provisioner/templates/provisioner-cluster-role-binding.yaml

            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: local-storage-provisioner-pv-binding
              labels:
                heritage: "Tiller"
                release: "release-name"
                chart: provisioner-2.3.2
            subjects:
            - kind: ServiceAccount
              name: local-storage-admin
              namespace: default
            roleRef:
              kind: ClusterRole
              name: system:persistent-volume-provisioner
              apiGroup: rbac.authorization.k8s.io
            ---
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRole
            metadata:
              name: local-storage-provisioner-node-clusterrole
              labels:
                heritage: "Tiller"
                release: "release-name"
                chart: provisioner-2.3.2
            rules:
            - apiGroups: [""]
              resources: ["nodes"]
              verbs: ["get"]
            ---
            apiVersion: rbac.authorization.k8s.io/v1
            kind: ClusterRoleBinding
            metadata:
              name: local-storage-provisioner-node-binding
              labels:
                heritage: "Tiller"
                release: "release-name"
                chart: provisioner-2.3.2
            subjects:
            - kind: ServiceAccount
              name: local-storage-admin
              namespace: default
            roleRef:
              kind: ClusterRole
              name: local-storage-provisioner-node-clusterrole
              apiGroup: rbac.authorization.k8s.io
            EOF

            kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.8.3/manifests/metallb.yaml

            cat <<EOF | kubectl apply -f -
            apiVersion: v1
            kind: ConfigMap
            metadata:
              namespace: metallb-system
              name: config
            data:
              config: |
                address-pools:
                - name: default
                  protocol: layer2
                  addresses:
                  - $KUBEFLOW_MASTER_IP_ADDRESS-$KUBEFLOW_MASTER_IP_ADDRESS
            EOF

            export KF_NAME=platiagro
            export BASE_DIR=$(pwd)
            export KF_DIR=${BASE_DIR}/${KF_NAME}
            export CONFIG_URI="https://raw.githubusercontent.com/platiagro/manifests/v0.0.2-kubeflow-v1.0-branch/kfdef/kfctl_platiagro.v0.0.2.yaml"
            mkdir -p ${KF_DIR}
            cd ${KF_DIR}
            kfctl apply -V -f ${CONFIG_URI}
      - name: Disconnect to VPN
        run: |
          sudo vpnc-disconnect
